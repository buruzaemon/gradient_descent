{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import gridspec\n",
    "from IPython.display import HTML\n",
    "\n",
    "plt.xkcd()\n",
    "\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".toggle_box {\n",
    "    position: fixed;\n",
    "    align: center;\n",
    "    padding: 2px;\n",
    "    top: 17%;\n",
    "    left: 94%;\n",
    "    opacity: 0.6;\n",
    "    z-index: 10000;\n",
    "}\n",
    ".toggle_button {\n",
    "    padding: 1px 3px 0 1px;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<script>\n",
    "var code_show = true;\n",
    "\n",
    "function code_toggle() {\n",
    "  if (code_show) {\n",
    "    $('div.input').hide('linear');\n",
    "    $('img#code-toggle')\n",
    "      .attr('src', 'assets/expand.png')\n",
    "      .attr('title', 'click to display code cells');\n",
    "  } else {\n",
    "    $('div.input').show('linear');\n",
    "    $('img#code-toggle')\n",
    "      .attr('src', 'assets/collapse.png')\n",
    "      .attr('title', 'click to hide code cells');\n",
    "  }\n",
    "  code_show = !code_show;\n",
    "}\n",
    "</script>\n",
    "\n",
    "<div class='toggle_box'>\n",
    "  <button class='toggle_button' onclick='javascript:code_toggle()'>\n",
    "    <img id='code-toggle' src='assets/collapse.png' title='click to hide code cells'/>\n",
    "  </button>\n",
    "</div>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent for Linear Regression\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In machine learning, gradient descent is used to optimize a cost function, by measuring the difference between a prediction $\\hat{y}$ given by a hypothesis function $h_{\\theta}$ and the actual value $y$. This difference between $h_{\\theta}$ and $y$ is modelled with cost function $J(\\theta)$.\n",
    "\n",
    "Optimizing $J(\\theta)$ helps us find a set of model parameters $\\theta$ that predict $y$ with the least amount of error.\n",
    "\n",
    "In terms of the graph below, gradient descent tries to find values for parameter $\\theta_{0}$ and $\\theta_{1}$ that minimize the cost surface $J(\\theta)$.\n",
    "\n",
    "![gradient descent](assets/3Dsurface.png \"What values for parameter θ₀ and θ₁ minimize the cost surface J(θ)?\")\n",
    "\n",
    "The notebook gives a very brief example of using gradient descent in linear regression.\n",
    "\n",
    "This example will cover:\n",
    "\n",
    "1. exploratory data analysis\n",
    "1. deciding on a hypothesis function\n",
    "1. deriving the corresponding cost function and gradient function\n",
    "1. implement gradient descent\n",
    "1. confirm and visualize the results\n",
    "\n",
    "The code in our example uses:\n",
    "\n",
    "* [NumPy](http://www.numpy.org/) - a package for scientific / mathematical computing\n",
    "* [Pandas](http://pandas.pydata.org/) - a Python data analysis library\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "We begin by loading our data on file into a [`pandas.Dataframe`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html).\n",
    "\n",
    "We can get a better idea about the data by using [`pandas.DataFrame.shape`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.shape.html) and [`pandas.DataFrame.head`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.head.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/ex1data1.txt',\n",
    "                   header=None,\n",
    "                   names=['x', 'y'])\n",
    "\n",
    "print('{} rows, {} columns\\n'.format(*data.shape))\n",
    "print('Here are the first 10 rows:\\n{}'.format(data.head(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('{}'.format(data.describe()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.y.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualing the data as $x,y$ coordinates in a scatter plot will help us decide on a hypothesis function for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,6))\n",
    "gs = gridspec.GridSpec(1, 2, width_ratios=[1,1])\n",
    "graph0 = plt.subplot(gs[0])\n",
    "graph0.scatter(data.x, data.y, color='#3182bd')\n",
    "graph0.set_xlabel('x', fontsize=14)\n",
    "graph0.set_ylabel('y', fontsize=14)\n",
    "graph0.set_title(r'Scatter plot of the dataset', fontsize=16)\n",
    "graph0.set_facecolor('0.88')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression and hypothesis function $h_{\\theta}$\n",
    "\n",
    "We would like to derive a model where, given a new value $x$, we could confidently predict the corresponding $y$. Gaining a clue from the scatter plot above, perhaps a _line that best fits the points in our data set_ will serve as the hypothesis function for our model.\n",
    "\n",
    "Recall that the slope-intercept formula for a straight line is\n",
    "\n",
    ">\\begin{align}\n",
    " y &= b + mx\n",
    ">\\end{align}\n",
    "\n",
    "The model we are looking for can then be represented with the following hypothesis function:\n",
    "\n",
    ">\\begin{align}\n",
    "  h_{\\theta}(x) &= \\theta_{0} + \\theta_{1}x\n",
    ">\\end{align}\n",
    "\n",
    "The model parameter in question, called collectively $\\theta$, are:\n",
    "* $\\theta_{0}$, $y$-intercept\n",
    "* $\\theta_{1}$, slope $m$\n",
    "\n",
    "But how do we go about finding appropriate values for $\\theta$ that will provide us the best possible model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing cost function $J(\\theta)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost(theta, X, y):\n",
    "    theta = np.matrix(theta)\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    \n",
    "    y_hat = np.dot(X, theta.T).sum(axis=1)\n",
    "    \n",
    "    cost = (1. / (2*m)) * np.square(y_hat - y).sum()\n",
    "    \n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient(theta, X, y):\n",
    "    theta = np.matrix(theta)\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    \n",
    "    y_hat = np.dot(X, theta.T).sum(axis=1)\n",
    "    err = (y_hat - y).ravel()\n",
    "    \n",
    "    deriv_0 = (1./m) * np.multiply(err, X[:, 0]).sum()\n",
    "    deriv_1 = (1./m) * np.multiply(err, X[:, 1]).sum()\n",
    "    \n",
    "    return deriv_0, deriv_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_ITERS = 1000    # number of iterations for gradient descent\n",
    "ALPHA = 1e-2        # step size scaling\n",
    "\n",
    "def gradient_descent(theta, X, y):\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    cost_history = np.zeros(shape=(NUM_ITERS, 1))\n",
    "    \n",
    "    theta_history = np.zeros(shape=(NUM_ITERS, 2))\n",
    "    \n",
    "    for i in range(NUM_ITERS):\n",
    "        deriv_0, deriv_1 = gradient(theta, X, y)\n",
    "        \n",
    "        theta[0] -= ALPHA * deriv_0\n",
    "        theta[1] -= ALPHA * deriv_1\n",
    "        \n",
    "        theta_history[i,0] = theta[0]\n",
    "        theta_history[i,1] = theta[1]\n",
    "        \n",
    "        cost_history[i,0] = cost(theta, X, y)\n",
    "        \n",
    "    return theta, cost_history, theta_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prepare training data, add bias column\n",
    "X = data.values[:,0:1]\n",
    "X = np.insert(X, 0, np.ones(X.shape[1]), axis=1)\n",
    "\n",
    "y = data.values[:,1:2]\n",
    "\n",
    "theta = np.zeros(2)\n",
    "\n",
    "# here are the first 5 rows of the training data\n",
    "print(X[0:5,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('With the following initial θ values:')\n",
    "print(r'θ₀ (y-intercept): {}'.format(theta[0]))\n",
    "print(r'θ₁ (slope): {}'.format(theta[1]))\n",
    "print('initial cost = {}'.format(cost(theta, X, y)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theta, cost_history, theta_history = gradient_descent(theta, X, y)\n",
    "\n",
    "print('After {} iterations of gradient descent:'.format(NUM_ITERS))\n",
    "print(r'θ₀ (y-intercept): {}'.format(theta[0]))\n",
    "print(r'θ₁ (slope): {}'.format(theta[1]))\n",
    "print('initial cost = {}'.format(cost(theta, X, y)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predict y value for x = 3.5 (adding in bias node value +1)\n",
    "predict1 = np.array([1, 3.5]).dot(theta)\n",
    "print('For x = 3.5, our model predict y = {}'.format(predict1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predict y value for x = 7.0 (adding in bias node value +1)\n",
    "predict2 = np.array([1, 7.0]).dot(theta)\n",
    "print('For x = 7.0, our model predict y = {}'.format(predict2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fx = theta_history[:,0]\n",
    "fb = theta_history[:,1]\n",
    "\n",
    "colseq = ['#fd8d3c', '#f16913', '#d94801', '#a63603', '#7f2704']\n",
    "\n",
    "plt.figure(figsize=(18,6))\n",
    "gs = gridspec.GridSpec(1, 2, width_ratios=[1,1])\n",
    "\n",
    "# scatter plot\n",
    "graph0 = plt.subplot(gs[0])\n",
    "graph0.scatter(X[:,1:2], y, color='#3182bd')\n",
    "\n",
    "for e in zip([0, 200, 400, 600, 800], [0.3, 0.4, 0.5, 0.6, 0.7]):\n",
    "    i, a = e\n",
    "    graph0.plot(data.values[:, 0:1],\n",
    "                X.dot([fx[i], fb[i]]).flatten(),\n",
    "                color=colseq[0],\n",
    "                alpha=a,\n",
    "                ls=':')\n",
    "\n",
    "result = X.dot(theta).flatten()\n",
    "graph0.plot(data.values[:,0:1], result, color='#7f2704')\n",
    "\n",
    "graph0.set_xlabel(r'x', fontsize=14)\n",
    "graph0.set_ylabel(r'y', fontsize=14)\n",
    "graph0.set_title(r'Optimizing $\\theta_0, \\theta_1$ with gradient descent',\n",
    "                 fontsize=16)\n",
    "graph0.set_facecolor('0.88')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,6))\n",
    "gs = gridspec.GridSpec(1, 2, width_ratios=[1,1])\n",
    "\n",
    "theta0_vals = np.linspace(-10, 10, 100)\n",
    "theta1_vals = np.linspace(-1, 4, 100)\n",
    "\n",
    "J_vals = np.zeros(shape=(theta0_vals.size, theta1_vals.size))\n",
    "\n",
    "for t1, e1 in enumerate(theta0_vals):\n",
    "    for t2, e2 in enumerate(theta1_vals):\n",
    "        thetaT = np.zeros(2)\n",
    "        thetaT[0] = e1\n",
    "        thetaT[1] = e2\n",
    "        J_vals[t1,t2] = cost(thetaT, X,  y)\n",
    "\n",
    "J_vals = J_vals.T\n",
    "\n",
    "graph1 = plt.subplot(gs[0])\n",
    "\n",
    "levels = np.logspace(-2, 3, 20).size\n",
    "colormap = plt.cm.get_cmap('coolwarm', levels-1)\n",
    "\n",
    "plt.contour(theta0_vals,\n",
    "            theta1_vals,\n",
    "            J_vals,\n",
    "            np.logspace(-2, 3, 20),\n",
    "            cmap=colormap)\n",
    "\n",
    "graph1.set_xlabel(r'$\\theta_0$', fontsize=14)\n",
    "graph1.set_ylabel(r'$\\theta_1$', fontsize=14)\n",
    "graph1.scatter(fx[0::200], fb[0::200], marker='x', c=colseq)\n",
    "graph1.scatter(theta[0], theta[1], color='#7f2704')\n",
    "graph1.set_title(r'Traversing contour with gradient descent on $J(\\theta_0,\\theta_1)$',\n",
    "                 fontsize=16)\n",
    "graph1.set_facecolor('0.88')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "res = minimize(cost,\n",
    "               np.zeros(2),\n",
    "               args=(X,y),\n",
    "               method='TNC',\n",
    "               jac=gradient)\n",
    "\n",
    "print('Using scipy.optimize.minimize(method=\"TNC\") function:')\n",
    "print(r'θ₀ (y-intercept): {}'.format(res.x[0]))\n",
    "print(r'θ₁ (slope): {}'.format(res.x[1]))\n",
    "print('cost = {}'.format(cost(res.x, X, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X[:, 1:2], y)\n",
    "\n",
    "theta0 = lr.intercept_.flatten()[0]\n",
    "theta1 = lr.coef_.flatten()[0]\n",
    "\n",
    "print('Using sklearn.linear_mode.LinearRegression:')\n",
    "print(r'θ₀ (y-intercept): {}'.format(theta0.item()))\n",
    "print(r'θ₁ (slope): {}'.format(theta1))\n",
    "print('cost = {}'.format(cost([theta0, theta1], X, y)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gramian = np.linalg.inv(np.dot(X.T, X))\n",
    "moment = np.dot(X.T, y)\n",
    "\n",
    "mpp_theta = np.dot(gramian, moment)\n",
    "\n",
    "print('Using OLS (Moore-Penrose pseudoinverse):')\n",
    "print(r'θ₀ (y-intercept): {}'.format(mpp_theta[0].item()))\n",
    "print(r'θ₁ (slope): {}'.format(mpp_theta[1].item()))\n",
    "print('cost = {}'.format(cost(mpp_theta.reshape((2)), X, y)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.rcParams['legend.fontsize'] = 18\n",
    "fig = plt.figure(figsize=(12,10))\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "x = theta0_vals\n",
    "y = theta1_vals\n",
    "z = J_vals\n",
    "\n",
    "ax.plot_surface(x, y, z,\n",
    "                rstride=2,\n",
    "                cstride=2,\n",
    "                cmap=plt.cm.get_cmap('coolwarm', levels-1),\n",
    "                linewidth=0.25,\n",
    "                antialiased=True)\n",
    "ax.set_xlabel(r'$\\theta_0$')\n",
    "ax.set_ylabel(r'$\\theta_1$')\n",
    "ax.set_zlabel(r'$J(\\theta_0)$')\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
